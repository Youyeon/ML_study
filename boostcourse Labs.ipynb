{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [부스트코스] 텐서플로우로 시작하는 딥러닝 기초\n",
    "\n",
    "# Lab 02 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "#W, b initialize\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "#learning_rate initialize\n",
    "#기울기를 얼마만큼 반영할 것인가를 결정하는 값\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    i\t     W\t\tb\tcost\n",
      "    0|    2.4520|     0.376| 45.660004\n",
      "    1|    2.1100|    0.2814| 26.608438\n",
      "    4|    1.4975|     0.112|  5.265920\n",
      "    5|    1.3813|   0.07987|  3.068760\n",
      "   16|    1.0256|  -0.01771|  0.008174\n",
      "   17|    1.0210|  -0.01889|  0.004804\n",
      "   20|    1.0128|  -0.02091|  0.001026\n",
      "   21|    1.0113|  -0.02127|  0.000637\n",
      "   32|    1.0063|  -0.02175|  0.000089\n",
      "   33|    1.0062|  -0.02169|  0.000087\n",
      "   36|    1.0060|   -0.0215|  0.000085\n",
      "   37|    1.0060|  -0.02143|  0.000084\n",
      "   48|    1.0057|  -0.02067|  0.000078\n",
      "   49|    1.0057|   -0.0206|  0.000078\n",
      "   52|    1.0056|  -0.02039|  0.000076\n",
      "   53|    1.0056|  -0.02032|  0.000076\n",
      "   64|    1.0054|  -0.01958|  0.000070\n",
      "   65|    1.0054|  -0.01951|  0.000070\n",
      "   68|    1.0053|  -0.01931|  0.000068\n",
      "   69|    1.0053|  -0.01925|  0.000068\n",
      "   80|    1.0051|  -0.01854|  0.000063\n",
      "   81|    1.0051|  -0.01848|  0.000063\n",
      "   84|    1.0051|  -0.01829|  0.000061\n",
      "   85|    1.0051|  -0.01823|  0.000061\n",
      "   96|    1.0049|  -0.01757|  0.000057\n",
      "   97|    1.0048|  -0.01751|  0.000056\n",
      "  100|    1.0048|  -0.01733|  0.000055\n"
     ]
    }
   ],
   "source": [
    "#Parameter(W, b) Update\n",
    "print(\"    i\\t     W\\t\\tb\\tcost\")\n",
    "for i in range(100+1):\n",
    "    # Gradient descent \n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W*x_data +b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate*W_grad)\n",
    "    b.assign_sub(learning_rate*b_grad)\n",
    "    if i&10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 Linear Regression and How to minimize cost\n",
    "## Cost function in pure Python - 비용 함수 구현하기\n",
    "\n",
    "### 비용 = 편차 제곱의 평균\n",
    "### 예측값 (=가정): $H(x) = Wx$ where W = gradient\n",
    "### 비용함수: $cost(W) = \\frac{1}{m} \\sum_{i=1}^m (W x_i - y_i)^2$ where m = len(W) = 데이터의 개수\n",
    "### 가정을 함수에 대입하면, $cost(W) = \\frac{1}{m} \\sum_{i=1}^m (H(x) - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow를 이용한 비용함수의 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_tf(W, X, Y):\n",
    "    hypothesis = X * W\n",
    "    return tf.reduce_mean(tr.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "W_values = np.linspace(-3, 5, num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "\n",
    "plt.plot(W_values, cost_values, \"b\")\n",
    "plt.ylabel('Cost(W)')\n",
    "plt.xlabel('W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow를 이용한 gradient 함수의 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent: $W = W - \\alpha\\frac{1}{m} \\sum_{i=1}^m (W(x_i) - y_i)x_i$\n",
    "### cost는 0으로 수렴, W는 특정 값으로 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) #for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "# randomly choose\n",
    "#W = tf.Variable(tf.random.normal([1], -100, 100))    #2 version\n",
    "# set particular num\n",
    "W = tf.Variable([5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  | cost       | W         \n",
      "    0 |    74.6667 |   4.813334\n",
      "   10 |    28.7093 |   3.364572\n",
      "   20 |    11.0387 |   2.466224\n",
      "   30 |     4.2444 |   1.909177\n",
      "   40 |     1.6320 |   1.563762\n",
      "   50 |     0.6275 |   1.349578\n",
      "   60 |     0.2413 |   1.216766\n",
      "   70 |     0.0928 |   1.134412\n",
      "   80 |     0.0357 |   1.083346\n",
      "   90 |     0.0137 |   1.051681\n",
      "  100 |     0.0053 |   1.032047\n",
      "  110 |     0.0020 |   1.019871\n",
      "  120 |     0.0008 |   1.012322\n",
      "  130 |     0.0003 |   1.007641\n",
      "  140 |     0.0001 |   1.004738\n",
      "  150 |     0.0000 |   1.002938\n",
      "  160 |     0.0000 |   1.001822\n",
      "  170 |     0.0000 |   1.001130\n",
      "  180 |     0.0000 |   1.000700\n",
      "  190 |     0.0000 |   1.000434\n",
      "  200 |     0.0000 |   1.000269\n",
      "  210 |     0.0000 |   1.000167\n",
      "  220 |     0.0000 |   1.000103\n",
      "  230 |     0.0000 |   1.000064\n",
      "  240 |     0.0000 |   1.000040\n",
      "  250 |     0.0000 |   1.000025\n",
      "  260 |     0.0000 |   1.000015\n",
      "  270 |     0.0000 |   1.000009\n",
      "  280 |     0.0000 |   1.000006\n",
      "  290 |     0.0000 |   1.000004\n"
     ]
    }
   ],
   "source": [
    "print('{:5} | {:10.4s} | {:10.6s}'.format('step', 'cost.numpy', 'W'))\n",
    "for step in range(300):\n",
    "    hypothesis = W*X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    \n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04 Multi-variable linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|x_1|x_2|x_3|y|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|73|80|75|152|\n",
    "|93|88|93|185|\n",
    "|89|91|90|180|\n",
    "|96|98|100|196|\n",
    "|73|66|70|142|\n",
    "\n",
    "## $H(x_1, x_2, x_3) = w_1x_1 + w_2x_2 + w_3x_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and Label\n",
    "x1 = [73., 93., 89., 96., 73.]\n",
    "x2 = [80., 88., 91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "y = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |    3528.1030\n",
      "   50 |      92.7332\n",
      "  100 |      54.4777\n",
      "  150 |      53.9173\n",
      "  200 |      53.7753\n",
      "  250 |      53.6384\n",
      "  300 |      53.5022\n",
      "  350 |      53.3662\n",
      "  400 |      53.2305\n",
      "  450 |      53.0953\n",
      "  500 |      52.9603\n",
      "  550 |      52.8258\n",
      "  600 |      52.6917\n",
      "  650 |      52.5578\n",
      "  700 |      52.4243\n",
      "  750 |      52.2911\n",
      "  800 |      52.1583\n",
      "  850 |      52.0259\n",
      "  900 |      51.8939\n",
      "  950 |      51.7621\n",
      " 1000 |      51.6306\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    #update w1, w2, w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    # x1, x2, x3, y\n",
    "    [73., 80., 75., 152.],\n",
    "    [93., 88., 93., 185.],\n",
    "    [89., 91., 90., 180.],\n",
    "    [96., 98., 100., 196.],\n",
    "    [73., 66., 70., 142.]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "# ,기준으로 앞 부분은 row, 뒷 부분은 column\n",
    "# ,으로 구분된 항목에서는 : 처음 행부터 끝행까지 , :-1 첫 column부터 마지막 column은 제외\n",
    "X = data[:, :-1]\n",
    "Y = data[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75.],\n",
       "       [ 93.,  88.,  93.],\n",
       "       [ 89.,  91.,  90.],\n",
       "       [ 96.,  98., 100.],\n",
       "       [ 73.,  66.,  70.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152.],\n",
       "       [185.],\n",
       "       [180.],\n",
       "       [196.],\n",
       "       [142.]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b # Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-257.55673],\n",
       "       [-286.47382],\n",
       "       [-294.48648],\n",
       "       [-317.55252],\n",
       "       [-215.21764]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost      \n",
      "    0 |    40.4728\n",
      "  100 |    40.2619\n",
      "  200 |    40.0522\n",
      "  300 |    39.8436\n",
      "  400 |    39.6359\n",
      "  500 |    39.4296\n",
      "  600 |    39.2244\n",
      "  700 |    39.0203\n",
      "  800 |    38.8172\n",
      "  900 |    38.6152\n",
      " 1000 |    38.4143\n",
      " 1100 |    38.2145\n",
      " 1200 |    38.0158\n",
      " 1300 |    37.8183\n",
      " 1400 |    37.6215\n",
      " 1500 |    37.4260\n",
      " 1600 |    37.2316\n",
      " 1700 |    37.0380\n",
      " 1800 |    36.8457\n",
      " 1900 |    36.6543\n",
      " 2000 |    36.4641\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print(\"{:5} | {:10.4s}\".format('epoch', 'cost'))\n",
    "for i in range(n_epochs+1):\n",
    "    # record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean(tf.square(predict(X) - Y))\n",
    "    \n",
    "    # calculates the gradients of the Loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    #update W and b\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Logistic Regression/Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]]\n",
    "y_train = [[0.], [0.], [0.], [1.], [1.], [1.]]\n",
    "\n",
    "x_test = [[5., 2.]]\n",
    "y_test = [[1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0]%3) for y in y_train]\n",
    "plt.scatter(x1, x2, c=colors, marker='^')\n",
    "plt.scatter(x_test[0][0], x_test[0][1], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"y1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow를 활용한 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $sigmoid(x) = \\frac{1}{1-e^(-x)}$\n",
    "## $cost(h(x), y) = ylog(h(x)) - (1-y)log(1-h(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수정의\n",
    "def logistic_regression(features):\n",
    "    hypothesis = tf.divide(1., 1. + tf.exp(-tf.matmul(features, W) + b)) #sigmoid func\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels*tf.math.log(hypothesis) + (1-labels)*tf.math.log(1-hypothesis))\n",
    "    return cost\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = logistic_regression(features)\n",
    "        loss_value = loss_fn(hypothesis, labels)\n",
    "    return tape.gradient(loss_value, [W, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 100, Loss: 0.5781\n",
      "Iter: 200, Loss: 0.5352\n",
      "Iter: 300, Loss: 0.5056\n",
      "Iter: 400, Loss: 0.4840\n",
      "Iter: 500, Loss: 0.4673\n",
      "Iter: 600, Loss: 0.4537\n",
      "Iter: 700, Loss: 0.4421\n",
      "Iter: 800, Loss: 0.4320\n",
      "Iter: 900, Loss: 0.4229\n",
      "Iter: 1000, Loss: 0.4145\n",
      "Test Result = [[1]]\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "EPOCHS = 1001\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in iter(dataset.batch(len(x_train))):\n",
    "        hypothesis = logistic_regression(features)\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "        if step%100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(hypothesis, labels)))\n",
    "\n",
    "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n",
    "print(\"Test Result = {}\".format(tf.cast(logistic_regression(x_test)>0.5, dtype=tf.int32)))\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6 Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "tf.random.set_seed(777)\n",
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "y_data = [[0,0,1],\n",
    "          [0,0,1],\n",
    "          [0,0,1],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [1,0,0],\n",
    "          [1,0,0]]\n",
    "\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "\n",
    "nb_classes = 3       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables=[W,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y*tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "def grad_fn(X,Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X,Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "def fit(X,Y,epochs=2000,verbose=100):\n",
    "    optimizer = optimizers.SGD(learning_rate=0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X,Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0)|((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X,Y).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9302204  0.06200533 0.00777428]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Softmax onehot test\n",
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y=x*x\n",
    "dy_dx = g.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.849417\n",
      "Loss at epoch 100: 0.684151\n",
      "Loss at epoch 200: 0.613813\n",
      "Loss at epoch 300: 0.558204\n",
      "Loss at epoch 400: 0.508306\n",
      "Loss at epoch 500: 0.461059\n",
      "Loss at epoch 600: 0.415072\n",
      "Loss at epoch 700: 0.369636\n",
      "Loss at epoch 800: 0.324533\n",
      "Loss at epoch 900: 0.280720\n",
      "Loss at epoch 1000: 0.246752\n",
      "Loss at epoch 1100: 0.232798\n",
      "Loss at epoch 1200: 0.221645\n",
      "Loss at epoch 1300: 0.211476\n",
      "Loss at epoch 1400: 0.202164\n",
      "Loss at epoch 1500: 0.193606\n",
      "Loss at epoch 1600: 0.185714\n",
      "Loss at epoch 1700: 0.178415\n",
      "Loss at epoch 1800: 0.171645\n",
      "Loss at epoch 1900: 0.165351\n",
      "Loss at epoch 2000: 0.159483\n"
     ]
    }
   ],
   "source": [
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00112886 0.08154673 0.9173244 ]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.1975952e-06 1.2331170e-03 9.9876475e-01]\n",
      " [1.1288594e-03 8.1546687e-02 9.1732442e-01]\n",
      " [2.2205539e-07 1.6418624e-01 8.3581358e-01]\n",
      " [6.3921816e-06 8.5045439e-01 1.4953922e-01]\n",
      " [2.6150808e-01 7.2644734e-01 1.2044534e-02]\n",
      " [1.3783246e-01 8.6214006e-01 2.7417480e-05]\n",
      " [7.4242145e-01 2.5754160e-01 3.6978410e-05]\n",
      " [9.2197549e-01 7.8023903e-02 6.0005692e-07]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2,1,3,2]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "b = hypothesis(x_data)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert as Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.472669\n",
      "Loss at epoch 500: 0.375229\n",
      "Loss at epoch 1000: 0.229923\n",
      "Loss at epoch 1500: 0.182147\n",
      "Loss at epoch 2000: 0.150633\n"
     ]
    }
   ],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)            \n",
    "            return grads\n",
    "    \n",
    "    def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            grads = self.grad_fn(X, Y)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))\n",
    "            \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-1 Neural Nets for XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels\n",
    "\n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "    layer3 = tf.concat([layer1, layer2], -1)\n",
    "    layer3 = tf.reshape(layer3, shape=[-1,2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([2,1]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([1]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([2,1]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([1]), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) #tf.train.GradientDescentOptimizer\n",
    "EPOCHS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.7824\n",
      "Iter: 5000, Loss: 0.6928\n",
      "Iter: 10000, Loss: 0.6890\n",
      "Iter: 15000, Loss: 0.6827\n",
      "Iter: 20000, Loss: 0.6697\n",
      "Iter: 25000, Loss: 0.6437\n",
      "Iter: 30000, Loss: 0.5864\n",
      "Iter: 35000, Loss: 0.4246\n",
      "Iter: 40000, Loss: 0.2294\n",
      "Iter: 45000, Loss: 0.1340\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for step in range(EPOCHS):\n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
    "        if step % 5000 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))\n",
    "            \n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab10 Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt :\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        checkpoint = tf.train.Checkpoint(dnn=model)\n",
    "        checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(ckpt_name.split('-')[1])\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0\n",
    "\n",
    "def check_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(): \n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data() #6만장, 1만장\n",
    "    \n",
    "    #batch_size, height, width, channel\n",
    "    #expand 함수로 channel 끝에 추가\n",
    "    train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "    test_data = np.expand_dims(test_data, axis=-1)\n",
    "    \n",
    "    train_data, test_data = normalize(train_data, test_data) #[0~255] -> [0~1]\n",
    "    \n",
    "    #One-hot Encoding\n",
    "    train_labels = to_categorical(train_labels, 10) #[N, ] -> [N. 10]\n",
    "    test_labels = to_categorical(test_labels, 10) # [N, ] -> [N, 10]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "def normalize(train_data, test_data):\n",
    "    train_data = train_data.astype(np.float32) / 255.0\n",
    "    test_data = test_data.astype(np.float32) / 255.0\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten():\n",
    "    return tf.keras.layers.Flatten()\n",
    "\n",
    "def dense(channel, weight_init):\n",
    "    return tf.keras.layers.Dense(units=channel, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "def relu():\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model(tf.keras.Model):\n",
    "    def __init__(self, label_dim):\n",
    "        super(create_model, self).__init__()\n",
    "        \n",
    "        weight_init = tf.keras.initializers.RandomNormal()\n",
    "        self.model = tf.keras.Sequential()\n",
    "        \n",
    "        self.model.add(flatten()) #[N, 28, 28, 1] -> [N, 784]\n",
    "        \n",
    "        for i in range(2):\n",
    "            #[N, 784] -> [N, 256] -> [N, 256]\n",
    "            self.model.add(dense(256, weight_init))\n",
    "            self.model.add(relu())\n",
    "        \n",
    "        self.model.add(dense(label_dim, weight_init))\n",
    "        \n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대상이 '7'이라면, label은 00000 00100이고, softmax 값은 7번자리가 가장 큰값 (총합 1)\n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)    \n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits, y_true=labels, from_logits=True))\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(model, images, labels):\n",
    "    #logit, label shape: [batch size, label_dim]\n",
    "    logits = model(images, training=False)\n",
    "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1)) #argmax: 숫자가 가장 큰 위치 리턴\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32)) #숫자로 변경\n",
    "    return accuracy\n",
    "\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "train_x, train_y, test_x, test_y = load_mnist()\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size\n",
    "\n",
    "label_dim = 10\n",
    "\n",
    "train_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Graph Input using Dataset API\"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size).\\\n",
    "    repeat()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x)).\\\n",
    "    repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model(label_dim)\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\"\"\" Writer \"\"\"\n",
    "checkpoint_dir = 'checkpoints'\n",
    "logs_dir = 'logs'\n",
    "\n",
    "model_dir = 'nn_softmax'\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "check_folder(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "logs_dir = os.path.join(logs_dir, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-07a882d3ed46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                     \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# and instead mimic ops placement in graphs: Operations on resource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2602\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2604\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_flag :\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(dnn=network)\n",
    "\n",
    "    # create writer for tensorboard\n",
    "    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\n",
    "    start_time = time()\n",
    "\n",
    "    # restore check-point if it exits\n",
    "    could_load, checkpoint_counter = load(network, checkpoint_dir)    \n",
    "\n",
    "    if could_load:\n",
    "        start_epoch = (int)(checkpoint_counter / training_iterations)        \n",
    "        counter = checkpoint_counter        \n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        start_iteration = 0\n",
    "        counter = 0\n",
    "        print(\" [!] Load failed...\")\n",
    "    \n",
    "    # train phase\n",
    "    with summary_writer.as_default():  # for tensorboard\n",
    "        for epoch in range(start_epoch, training_epochs):\n",
    "            for idx, (train_input, train_label) in enumerate(train_dataset):            \n",
    "                grads = grad(network, train_input, train_label)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "\n",
    "                train_loss = loss_fn(network, train_input, train_label)\n",
    "                train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "                \n",
    "                for test_input, test_label in test_dataset:                \n",
    "                    test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "                tf.summary.scalar(name='train_loss', data=train_loss, step=counter)\n",
    "                tf.summary.scalar(name='train_accuracy', data=train_accuracy, step=counter)\n",
    "                tf.summary.scalar(name='test_accuracy', data=test_accuracy, step=counter)\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\n",
    "                       test_accuracy))\n",
    "                counter += 1                \n",
    "        checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\n",
    "        \n",
    "# test phase      \n",
    "else :\n",
    "    _, _ = load(network, checkpoint_dir)\n",
    "    for test_input, test_label in test_dataset:    \n",
    "        test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "    print(\"test_Accuracy: %.4f\" % (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
